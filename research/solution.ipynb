{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\Chinelo\\\\Data-Chatbot\\\\research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\Chinelo\\\\Data-Chatbot'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import faiss\n",
    "import numpy as np\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.document_loaders import PyPDFLoader, TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.schema import HumanMessage, SystemMessage, AIMessage\n",
    "from dotenv import load_dotenv\n",
    "from docx import Document \n",
    "from PyPDF2 import PdfReader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()\n",
    "os.environ['GROQ_API_KEY'] = os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_from_pdf(pdf_docs):\n",
    "    \"\"\"Extract text from uploaded PDF files.\"\"\"\n",
    "    text = \"\"\n",
    "    for pdf in pdf_docs:\n",
    "        pdf_reader = PdfReader(pdf)\n",
    "        for page in pdf_reader.pages:\n",
    "            text += page.extract_text()\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_from_docx(docx_docs):\n",
    "    \"\"\"Extract text from uploaded DOCX files.\"\"\"\n",
    "    text = \"\"\n",
    "    for docx in docx_docs:\n",
    "        doc = Document(docx)\n",
    "        for paragraph in doc.paragraphs:\n",
    "            text += paragraph.text + \"\\n\"  \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_chunks(text):\n",
    "    \"\"\"Split text into manageable chunks.\"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=10000, chunk_overlap=1000)\n",
    "    chunks = text_splitter.split_text(text)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vector_store(text_chunks):\n",
    "    \"\"\"Create vector store and save it locally.\"\"\"\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "    vector_store = FAISS.from_texts(text_chunks, embedding=embeddings)\n",
    "    vector_store.save_local(\"faiss_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_documents(pdf_docs, docx_docs):\n",
    "    \"\"\"Process both PDF and DOCX documents.\"\"\"\n",
    "    # Extract text from PDF and DOCX files\n",
    "    pdf_text = get_text_from_pdf(pdf_docs) if pdf_docs else \"\"\n",
    "    docx_text = get_text_from_docx(docx_docs) if docx_docs else \"\"\n",
    "\n",
    "    # Combine the extracted text from both PDF and DOCX files\n",
    "    full_text = pdf_text + docx_text\n",
    "\n",
    "    # Split the combined text into chunks\n",
    "    text_chunks = get_text_chunks(full_text)\n",
    "\n",
    "    # Create a vector store\n",
    "    get_vector_store(text_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_conversational_chain():\n",
    "    prompt_template = \"\"\"\n",
    "    Answer the question as detailed as possible from the provided context, make sure to provide all the details, if the answer is not in\n",
    "    provided context just say, \"answer is not available in the context\", don't provide the wrong answer\\n\\n\n",
    "\n",
    "    2. **Documents**: \n",
    "   - Review the documents thoroughly and focus only on the information presented in them. \n",
    "   - Do not introduce any external information not found in the provided documents.\n",
    "   - If there is insufficient information to provide a direct answer, respond with:\n",
    "     \"Sorry, I couldn't find sufficient information in the provided documents to answer your question. Please ensure that your query is related to the document context, or provide more specific details.\"\n",
    "\n",
    "3. **Response Guidelines**:\n",
    "   - Provide clear, accurate, and concise answers, directly addressing the userâ€™s query with only the information derived from the documents.\n",
    "   - Maintain a professional tone, using terminology from the documents when appropriate, but ensure that your response is easily understandable to users with basic to intermediate knowledge of the content.\n",
    "   - If the response requires technical terms, make sure to define them or explain them in simple language.\n",
    "   - Always use a tone that is respectful, empathetic, and informative. Make sure the user feels heard and supported.\n",
    "   \n",
    "4. **When to Ask for Clarification**:\n",
    "   - If the user's query is ambiguous, overly broad, or could refer to multiple topics, ask the user for further clarification to provide a more accurate response.\n",
    "   - Examples of follow-up questions: \n",
    "     \"Could you please clarify what you mean by [specific term]?\"\n",
    "     \"Can you provide more details or context regarding [specific topic]?\"\n",
    "\n",
    "    Context:\\n {context}?\\n\n",
    "    Question: \\n{question}\\n  Answer:\n",
    "    \"\"\"\n",
    "    # Here, you need to use llm (which is defined earlier) instead of model.\n",
    "    llm = ChatGroq(model_name=\"llama-3.3-70b-versatile\", temperature=0.5)\n",
    "    \n",
    "    prompt = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
    "    \n",
    "    # Use llm instead of model when calling load_qa_chain\n",
    "    chain = load_qa_chain(llm, chain_type=\"stuff\", prompt=prompt)\n",
    "    \n",
    "    return chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_input(user_question):\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "    \n",
    "    new_db = FAISS.load_local(\"faiss_index\", embeddings)\n",
    "    docs = new_db.similarity_search(user_question)\n",
    "\n",
    "    chain = get_conversational_chain()\n",
    "\n",
    "    \n",
    "    response = chain(\n",
    "        {\"input_documents\":docs, \"question\": user_question}\n",
    "        , return_only_outputs=True)\n",
    "\n",
    "    print(response)\n",
    "    st.write(\"Reply: \", response[\"output_text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting text_processing\n",
      "  Downloading text_processing-0.0.2.tar.gz (2.9 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Building wheels for collected packages: text_processing\n",
      "  Building wheel for text_processing (setup.py): started\n",
      "  Building wheel for text_processing (setup.py): finished with status 'done'\n",
      "  Created wheel for text_processing: filename=text_processing-0.0.2-py3-none-any.whl size=3693 sha256=a89b49e073c14202168399412ae50a1892526b2aa9bcd3c6aa7c94e862342ed1\n",
      "  Stored in directory: c:\\users\\chinelo\\appdata\\local\\pip\\cache\\wheels\\e6\\b0\\52\\417909c7fa7a88fa0251391552118fecfb64107c03203c4917\n",
      "Successfully built text_processing\n",
      "Installing collected packages: text_processing\n",
      "Successfully installed text_processing-0.0.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install text_processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nbimporterNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading nbimporter-0.3.4-py3-none-any.whl.metadata (252 bytes)\n",
      "Downloading nbimporter-0.3.4-py3-none-any.whl (4.9 kB)\n",
      "Installing collected packages: nbimporter\n",
      "Successfully installed nbimporter-0.3.4\n"
     ]
    }
   ],
   "source": [
    "pip install nbimporter\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "databot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
